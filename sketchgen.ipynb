{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import six\n",
    "from six.moves import cStringIO as StringIO\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback, LearningRateScheduler, TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator to feed into Keras' fit_generator for loading of data\n",
    "def batch_generator(dataset, train):\n",
    "    count = 0  # batch counter for validation\\test data\n",
    "    while True:\n",
    "        if train:\n",
    "            _, batch, s = dataset.random_batch()\n",
    "\n",
    "        else:  # validation\\test data\n",
    "            count = 0 if count == dataset.num_batches else count\n",
    "            _, batch, s = dataset.get_batch(count)\n",
    "            count += 1\n",
    "\n",
    "        encoder_input = batch[:, 1:dataset.max_seq_length + 1, :]\n",
    "        # The expected vectors of strokes\n",
    "        target_output = encoder_input\n",
    "\n",
    "        # Vectors of strokes to be fed to decoder (same as above, but lagged behind\n",
    "        # one step to include initial dummy value of (0, 0, 1, 0, 0))\n",
    "        decoder_input = batch[:, :dataset.max_seq_length, :]\n",
    "\n",
    "        yield ({'encoder_input': encoder_input, 'decoder_input': decoder_input}, {'output': target_output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, model_params):\n",
    "    \"\"\"Loads the .npz file, and splits the set into train/valid/test.\"\"\"\n",
    "\n",
    "    #   normalizes the x and y columns using the training set.\n",
    "    # applies same scaling factor to valid and test set.\n",
    "\n",
    "    if isinstance(model_params.data_set, list):\n",
    "        datasets = model_params.data_set\n",
    "    else:\n",
    "        datasets = [model_params.data_set]\n",
    "\n",
    "    train_strokes = None\n",
    "    valid_strokes = None\n",
    "    test_strokes = None\n",
    "\n",
    "    for dataset in datasets:\n",
    "        data_filepath = os.path.join(data_dir, dataset)\n",
    "        if data_dir.startswith('http://') or data_dir.startswith('https://'):\n",
    "            print('Downloading %s', data_filepath)\n",
    "            response = requests.get(data_filepath)\n",
    "            data = np.load(StringIO(response.content))\n",
    "        else:\n",
    "            if six.PY3:\n",
    "                data = np.load(data_filepath, encoding='latin1')\n",
    "            else:\n",
    "                data = np.load(data_filepath)\n",
    "        print('Loaded {}/{}/{} from {}'.format(\n",
    "            len(data['train']), len(data['valid']), len(data['test']),\n",
    "            dataset))\n",
    "\n",
    "        if train_strokes is None:\n",
    "            train_strokes = data['train']\n",
    "            valid_strokes = data['valid']\n",
    "            test_strokes = data['test']\n",
    "        else:\n",
    "            train_strokes = np.concatenate((train_strokes, data['train']))\n",
    "            valid_strokes = np.concatenate((valid_strokes, data['valid']))\n",
    "            test_strokes = np.concatenate((test_strokes, data['test']))\n",
    "\n",
    "    all_strokes = np.concatenate((train_strokes, valid_strokes, test_strokes))\n",
    "    num_points = 0\n",
    "    for stroke in all_strokes:\n",
    "        num_points += len(stroke)\n",
    "    avg_len = num_points / len(all_strokes)\n",
    "    print('Dataset combined: {} ({}/{}/{}), avg len {}'.format(\n",
    "        len(all_strokes), len(train_strokes), len(valid_strokes),\n",
    "        len(test_strokes), int(avg_len)))\n",
    "\n",
    "    # calculates the max strokes we need.\n",
    "    max_seq_len = get_max_len(all_strokes)\n",
    "    #overwrites the given values\n",
    "    model_params.max_seq_len = max_seq_len\n",
    "\n",
    "    print('model_params.max_seq_len:', int(model_params.max_seq_len))\n",
    "\n",
    "    train_set = DataLoader(\n",
    "        train_strokes,\n",
    "        model_params.batch_size,\n",
    "        max_seq_length=model_params.max_seq_len,\n",
    "        random_scale_factor=model_params.random_scale_factor,\n",
    "        augment_stroke_prob=model_params.augment_stroke_prob)\n",
    "\n",
    "    normalizing_scale_factor = train_set.calculate_normalizing_scale_factor()\n",
    "    train_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "    valid_set = DataLoader(\n",
    "        valid_strokes,\n",
    "        model_params.batch_size,\n",
    "        max_seq_length=model_params.max_seq_len,\n",
    "        random_scale_factor=0.0,\n",
    "        augment_stroke_prob=0.0)\n",
    "    valid_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "    test_set = DataLoader(\n",
    "        test_strokes,\n",
    "        model_params.batch_size,\n",
    "        max_seq_length=model_params.max_seq_len,\n",
    "        random_scale_factor=0.0,\n",
    "        augment_stroke_prob=0.0)\n",
    "    test_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "    print('normalizing_scale_factor ', normalizing_scale_factor)\n",
    "\n",
    "    result = [train_set, valid_set, test_set, model_params]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logger class to enable logging to file and terminal together \n",
    "class Logger(object):\n",
    "    def __init__(self, logsdir):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(os.path.join(logsdir, 'log.txt'), \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot.notation access to dictionary attributes\n",
    "class DotDict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return DotDict([(copy.deepcopy(k, memo), copy.deepcopy(v, memo)) for k, v in self.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback class to modify the default learning rate scheduler to operate each batch\n",
    "class LearningRateSchedulerPerBatch(LearningRateScheduler):\n",
    "    def __init__(self, schedule, verbose=0):\n",
    "        super(LearningRateSchedulerPerBatch, self).__init__(schedule, verbose)\n",
    "        self.count = 0  \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        super(LearningRateSchedulerPerBatch, self).on_epoch_begin(self.count, logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        super(LearningRateSchedulerPerBatch, self).on_epoch_end(self.count, logs)\n",
    "        self.count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLWeightScheduler(Callback):\n",
    "    \"\"\"KL weight scheduler.\n",
    "    # Arguments\n",
    "        kl_weight: The tensor withholding the current KL weight term\n",
    "        schedule: a function that takes a batch index as input\n",
    "            (integer, indexed from 0) and returns a new learning rate as output (float).\n",
    "        verbose: int. 0: quiet, 1: update messages.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kl_weight, schedule, verbose=0):\n",
    "        super(KLWeightScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "        self.verbose = verbose\n",
    "        self.kl_weight = kl_weight\n",
    "        self.count = 0 \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "\n",
    "        new_kl_weight = self.schedule(self.count)\n",
    "        if not isinstance(new_kl_weight, (float, np.float32, np.float64)):\n",
    "            raise ValueError('The output of the \"schedule\" function '\n",
    "                             'should be float.')\n",
    "        # Set new value\n",
    "        K.set_value(self.kl_weight, new_kl_weight)\n",
    "        if self.verbose > 0 and self.count % 20 == 0:\n",
    "            print('\\nBatch %05d: KLWeightScheduler setting KL weight '\n",
    "                  ' to %s.' % (self.count + 1, new_kl_weight))\n",
    "        self.count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modification is done to TensorBoard callback to include learning rate and kl weights \n",
    "class TensorBoardLR(TensorBoard):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.kl_weight = kwargs.pop('kl_weight')\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.count = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs.update({'lr': K.eval(self.model.optimizer.lr),\n",
    "                     'kl_weight': K.eval(self.kl_weight)})\n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns bounds of data.\n",
    "def get_bounds(data, factor=10):\n",
    "    min_x = 0\n",
    "    max_x = 0\n",
    "    min_y = 0\n",
    "    max_y = 0\n",
    "\n",
    "    abs_x = 0\n",
    "    abs_y = 0\n",
    "    for i in range(len(data)):\n",
    "        x = float(data[i, 0]) / factor\n",
    "        y = float(data[i, 1]) / factor\n",
    "        abs_x += x\n",
    "        abs_y += y\n",
    "        min_x = min(min_x, abs_x)\n",
    "        min_y = min(min_y, abs_y)\n",
    "        max_x = max(max_x, abs_x)\n",
    "        max_y = max(max_y, abs_y)\n",
    "\n",
    "    return (min_x, max_x, min_y, max_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spherical interpolation.\n",
    "def slerp(p0, p1, t):\n",
    "    omega = np.arccos(np.dot(p0 / np.linalg.norm(p0), p1 / np.linalg.norm(p1)))\n",
    "    so = np.sin(omega)\n",
    "    return np.sin((1.0 - t) * omega) / so * p0 + np.sin(t * omega) / so * p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear interpolation.\n",
    "def lerp(p0, p1, t):\n",
    "    return (1.0 - t) * p0 + t * p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert stroke-3 format to polyline format.\n",
    "def strokes_to_lines(strokes):\n",
    "    x = 0\n",
    "    y = 0\n",
    "    lines = []\n",
    "    line = []\n",
    "    for i in range(len(strokes)):\n",
    "        if strokes[i, 2] == 1:\n",
    "            x += float(strokes[i, 0])\n",
    "            y += float(strokes[i, 1])\n",
    "            line.append([x, y])\n",
    "            lines.append(line)\n",
    "            line = []\n",
    "        else:\n",
    "            x += float(strokes[i, 0])\n",
    "            y += float(strokes[i, 1])\n",
    "            line.append([x, y])\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert polyline format to stroke-3 format.\n",
    "def lines_to_strokes(lines):\n",
    "    eos = 0\n",
    "    strokes = [[0, 0, 0]]\n",
    "    for line in lines:\n",
    "        linelen = len(line)\n",
    "        for i in range(linelen):\n",
    "            eos = 0 if i < linelen - 1 else 1\n",
    "            strokes.append([line[i][0], line[i][1], eos])\n",
    "    strokes = np.array(strokes)\n",
    "    strokes[1:, 0:2] -= strokes[:-1, 0:2]\n",
    "    return strokes[1:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform data augmentation by randomly dropping out strokes.\n",
    "def augment_strokes(strokes, prob=0.0):\n",
    "    # drop each point within a line segments with a probability of prob\n",
    "    # note that the logic in the loop prevents points at the ends to be dropped.\n",
    "    result = []\n",
    "    prev_stroke = [0, 0, 1]\n",
    "    count = 0\n",
    "    stroke = [0, 0, 1]  # Added to be safe.\n",
    "    for i in range(len(strokes)):\n",
    "        candidate = [strokes[i][0], strokes[i][1], strokes[i][2]]\n",
    "        if candidate[2] == 1 or prev_stroke[2] == 1:\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "        urnd = np.random.rand()  # uniform random variable\n",
    "        if candidate[2] == 0 and prev_stroke[2] == 0 and count > 2 and urnd < prob:\n",
    "            stroke[0] += candidate[0]\n",
    "            stroke[1] += candidate[1]\n",
    "        else:\n",
    "            stroke = candidate\n",
    "            prev_stroke = stroke\n",
    "            result.append(stroke)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale an entire image to be less than a certain size.\n",
    "def scale_bound(stroke, average_dimension=10.0):\n",
    "    # stroke is a numpy array of [dx, dy, pstate], average_dimension is a float.\n",
    "    # modifies stroke directly.\n",
    "    bounds = get_bounds(stroke, 1)\n",
    "    max_dimension = max(bounds[1] - bounds[0], bounds[3] - bounds[2])\n",
    "    stroke[:, 0:2] /= (max_dimension / average_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from stroke-5 format (from sketch-rnn paper) back to stroke-3.\n",
    "def to_normal_strokes(big_stroke):\n",
    "    l = 0\n",
    "    for i in range(len(big_stroke)):\n",
    "        if big_stroke[i, 4] > 0:\n",
    "            l = i\n",
    "            break\n",
    "    if l == 0:\n",
    "        l = len(big_stroke)\n",
    "    result = np.zeros((l, 3))\n",
    "    result[:, 0:2] = big_stroke[0:l, 0:2]\n",
    "    result[:, 2] = big_stroke[0:l, 3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuts unnecessary endpoints\n",
    "def clean_strokes(sample_strokes, factor=100):\n",
    "    copy_stroke = []\n",
    "    added_final = False\n",
    "    for j in range(len(sample_strokes)):\n",
    "        finish_flag = int(sample_strokes[j][4])\n",
    "        if finish_flag == 0:\n",
    "            copy_stroke.append([\n",
    "                int(round(sample_strokes[j][0] * factor)),\n",
    "                int(round(sample_strokes[j][1] * factor)),\n",
    "                int(sample_strokes[j][2]),\n",
    "                int(sample_strokes[j][3]), finish_flag\n",
    "            ])\n",
    "        else:\n",
    "            copy_stroke.append([0, 0, 0, 0, 1])\n",
    "            added_final = True\n",
    "            break\n",
    "    if not added_final:\n",
    "        copy_stroke.append([0, 0, 0, 0, 1])\n",
    "    return copy_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts from stroke-3 to stroke-5 format and pads to given length.\n",
    "def to_big_strokes(stroke, max_len=250):\n",
    "    result = np.zeros((max_len, 5), dtype=float)\n",
    "    l = len(stroke)\n",
    "    assert l <= max_len\n",
    "    result[0:l, 0:2] = stroke[:, 0:2]\n",
    "    result[0:l, 3] = stroke[:, 2]\n",
    "    result[0:l, 2] = 1 - result[0:l, 3]\n",
    "    result[l:, 4] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the maximum length of an array of strokes.\n",
    "def get_max_len(strokes):\n",
    "    max_len = 0\n",
    "    for stroke in strokes:\n",
    "        ml = len(stroke)\n",
    "        if ml > max_len:\n",
    "            max_len = ml\n",
    "    return max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for loading data.\n",
    "class DataLoader(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 strokes,\n",
    "                 batch_size=100,\n",
    "                 max_seq_length=250,\n",
    "                 scale_factor=1.0,\n",
    "                 random_scale_factor=0.0,\n",
    "                 augment_stroke_prob=0.0,\n",
    "                 limit=1000):\n",
    "        self.batch_size = batch_size  # minibatch size\n",
    "        self.max_seq_length = max_seq_length  # N_max in sketch-rnn paper\n",
    "        self.scale_factor = scale_factor  # divide offsets by this factor\n",
    "        self.random_scale_factor = random_scale_factor  # data augmentation method\n",
    "        # Removes large gaps in the data. x and y offsets are clamped to have\n",
    "        # absolute value no greater than this limit.\n",
    "        self.limit = limit\n",
    "        self.augment_stroke_prob = augment_stroke_prob  # data augmentation method\n",
    "        self.start_stroke_token = [0, 0, 1, 0, 0]  # S_0 in sketch-rnn paper\n",
    "        # sets self.strokes (list of ndarrays, one per sketch, in stroke-3 format,\n",
    "        # sorted by size)\n",
    "        self.preprocess(strokes)\n",
    "\n",
    "    #Remove entries from strokes having > max_seq_length points.\n",
    "    def preprocess(self, strokes):\n",
    "        raw_data = []\n",
    "        seq_len = []\n",
    "        count_data = 0\n",
    "\n",
    "        for i in range(len(strokes)):\n",
    "            data = strokes[i]\n",
    "            if len(data) <= (self.max_seq_length):\n",
    "                count_data += 1\n",
    "                # removes large gaps from the data\n",
    "                data = np.minimum(data, self.limit)\n",
    "                data = np.maximum(data, -self.limit)\n",
    "                data = np.array(data, dtype=np.float32)\n",
    "                data[:, 0:2] /= self.scale_factor\n",
    "                raw_data.append(data)\n",
    "                seq_len.append(len(data))\n",
    "        seq_len = np.array(seq_len)  # nstrokes for each sketch\n",
    "        idx = np.argsort(seq_len)\n",
    "        self.strokes = []\n",
    "        for i in range(len(seq_len)):\n",
    "            self.strokes.append(raw_data[idx[i]])\n",
    "        print(\"total images <= max_seq_len is %d\" % count_data)\n",
    "        self.num_batches = int(count_data / self.batch_size)\n",
    "\n",
    "    #Returns a random sample, in stroke-3 format\n",
    "    def random_sample(self):\n",
    "        sample = np.copy(random.choice(self.strokes))\n",
    "        return sample\n",
    "\n",
    "    #Augments data by stretching x and y axis randomly [1-e, 1+e].\n",
    "    def random_scale(self, data):\n",
    "        x_scale_factor = (\n",
    "            np.random.random() - 0.5) * 2 * self.random_scale_factor + 1.0\n",
    "        y_scale_factor = (\n",
    "            np.random.random() - 0.5) * 2 * self.random_scale_factor + 1.0\n",
    "        result = np.copy(data)\n",
    "        result[:, 0] *= x_scale_factor\n",
    "        result[:, 1] *= y_scale_factor\n",
    "        return result\n",
    "\n",
    "    #Calculates the normalizing factor explained in appendix of sketch-rnn.\n",
    "    def calculate_normalizing_scale_factor(self):\n",
    "        data = []\n",
    "        for i in range(len(self.strokes)):\n",
    "            if len(self.strokes[i]) > self.max_seq_length:\n",
    "                continue\n",
    "            for j in range(len(self.strokes[i])):\n",
    "                data.append(self.strokes[i][j, 0])\n",
    "                data.append(self.strokes[i][j, 1])\n",
    "        data = np.array(data)\n",
    "        return np.std(data)\n",
    "\n",
    "    #Normalizes entire dataset (delta_x, delta_y) by the scaling factor.\n",
    "    def normalize(self, scale_factor=None):\n",
    "        if scale_factor is None:\n",
    "            scale_factor = self.calculate_normalizing_scale_factor()\n",
    "        self.scale_factor = scale_factor\n",
    "        for i in range(len(self.strokes)):\n",
    "            self.strokes[i][:, 0:2] /= self.scale_factor\n",
    "            \n",
    "    #Returns the potentially augmented batch for a list of indices\n",
    "    def _get_batch_from_indices(self, indices):    \n",
    "        x_batch = []\n",
    "        seq_len = []\n",
    "        for idx in range(len(indices)):\n",
    "            i = indices[idx]\n",
    "            data = self.random_scale(self.strokes[i])\n",
    "            data_copy = np.copy(data)\n",
    "            if self.augment_stroke_prob > 0:\n",
    "                data_copy = augment_strokes(data_copy, self.augment_stroke_prob)\n",
    "            x_batch.append(data_copy)\n",
    "            length = len(data_copy)\n",
    "            seq_len.append(length)\n",
    "        seq_len = np.array(seq_len, dtype=int)\n",
    "        # We return three things: stroke-3 format, stroke-5 format, list of seq_len.\n",
    "        return x_batch, self.pad_batch(x_batch, self.max_seq_length), seq_len\n",
    "\n",
    "    #Return a randomised portion of the training data.\n",
    "    def random_batch(self):\n",
    "        idx = np.random.permutation(range(0, len(self.strokes)))[0:self.batch_size]\n",
    "        return self._get_batch_from_indices(idx)\n",
    "\n",
    "    #Get the idx'th batch from the dataset.\n",
    "    def get_batch(self, idx):\n",
    "        # print('DBG'+str(idx))\n",
    "        assert idx >= 0, \n",
    "        assert idx < self.num_batches, \n",
    "        start_idx = idx * self.batch_size\n",
    "        indices = range(start_idx, start_idx + self.batch_size)\n",
    "        return self._get_batch_from_indices(indices)\n",
    "\n",
    "    #Pads the batch to be stroke-5 bigger format\n",
    "    def pad_batch(self, batch, max_len):\n",
    "        result = np.zeros((self.batch_size, max_len + 1, 5), dtype=float)\n",
    "        assert len(batch) == self.batch_size\n",
    "        for i in range(self.batch_size):\n",
    "            l = len(batch[i])\n",
    "            assert l <= max_len\n",
    "            result[i, 0:l, 0:2] = batch[i][:, 0:2]\n",
    "            result[i, 0:l, 3] = batch[i][:, 2]\n",
    "            result[i, 0:l, 2] = 1 - result[i, 0:l, 3]\n",
    "            result[i, l:, 4] = 1\n",
    "            # put in the first token, as described in sketch-rnn methodology\n",
    "            result[i, 1:, :] = result[i, :-1, :]\n",
    "            result[i, 0, :] = 0\n",
    "            result[i, 0, 2] = self.start_stroke_token[2]  \n",
    "            result[i, 0, 3] = self.start_stroke_token[3]\n",
    "            result[i, 0, 4] = self.start_stroke_token[4]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
